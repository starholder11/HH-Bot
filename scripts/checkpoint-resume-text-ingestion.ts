#!/usr/bin/env tsx

import { ParallelIngestionService } from '../lib/parallel-ingestion';
import { LanceDBIngestionService } from '../lib/lancedb-ingestion-backup';
import { chunkText } from '../lib/chunk-utils';
import './bootstrap-env';

interface ProcessedDocument {
  slug: string;
  chunkCount: number;
  lastProcessedChunk: number;
  status: 'completed' | 'partial' | 'failed';
}

async function checkExistingTextDocuments(lancedbUrl: string): Promise<Set<string>> {
  console.log('üîç Checking which text documents are already in LanceDB...');

  try {
    // Search for all text records with empty query to get them all
    const response = await fetch(`${lancedbUrl}/search`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        query: '',
        vector: new Array(1536).fill(0), // dummy vector
        limit: 50000, // Large limit to get all records
        filter: "content_type = 'text'"
      })
    });

    if (!response.ok) {
      console.warn(`‚ö†Ô∏è  Could not check existing documents: ${response.status}`);
      return new Set();
    }

    const results = await response.json();
    const existingDocs = new Set<string>();

    // Extract unique document slugs from text chunk IDs
    for (const result of results) {
      if (result.id && result.id.startsWith('text_')) {
        // Extract slug from ID like "text_slug#123"
        const match = result.id.match(/^text_([^#]+)/);
        if (match) {
          existingDocs.add(match[1]);
        }
      }
    }

    console.log(`‚úÖ Found ${existingDocs.size} text documents already in LanceDB`);
    return existingDocs;

  } catch (error) {
    console.warn(`‚ö†Ô∏è  Could not check existing documents:`, error.message);
    return new Set();
  }
}

async function main() {
  console.log('üîÑ CHECKPOINT-AWARE TEXT INGESTION');
  console.log('‚ö° Resumes from where previous runs left off');
  console.log('üìã Only processes missing or incomplete documents');

  const parallelService = new ParallelIngestionService();
  const legacyService = new LanceDBIngestionService();
  const LANCEDB_API_URL = process.env.LANCEDB_API_URL || 'http://localhost:8000';

  try {
    // Step 1: Check current state
    console.log('\\nüìä Checking current LanceDB state...');
    const countResponse = await fetch(`${LANCEDB_API_URL}/count`);
    if (countResponse.ok) {
      const { count } = await countResponse.json();
      console.log(`üìä Current LanceDB records: ${count}`);
    }

    // Step 2: Load all text content from GitHub
    console.log('\\nüìÑ Loading text content from GitHub...');
    const textContents = await legacyService.loadTextContent();
    console.log(`‚úÖ Loaded ${textContents.length} text documents from GitHub`);

    // Step 3: Check which documents are already processed
    const existingDocs = await checkExistingTextDocuments(LANCEDB_API_URL);

    // Step 4: Filter to only documents that need processing
    const docsToProcess = textContents.filter(doc => !existingDocs.has(doc.slug));

    console.log(`\\nüéØ Processing strategy:`);
    console.log(`   üìö Total documents: ${textContents.length}`);
    console.log(`   ‚úÖ Already processed: ${existingDocs.size}`);
    console.log(`   üîÑ Need processing: ${docsToProcess.length}`);

    if (docsToProcess.length === 0) {
      console.log('\\nüéâ All text documents are already processed!');
      console.log('‚úÖ Text ingestion is complete');
      return;
    }

    // Step 5: Process only the missing documents
    console.log(`\\n‚ö° Processing ${docsToProcess.length} missing documents...`);

    // Convert documents to content items with chunks
    const contentItems = [];
    let totalChunks = 0;

    for (const doc of docsToProcess) {
      const chunks = chunkText(doc.content);
      totalChunks += chunks.length;

      for (const chunk of chunks) {
        contentItems.push({
          id: `text_${doc.slug}#${chunk.ix}`,
          title: doc.title,
          content_type: 'text',
          combinedText: chunk.text,
          metadata: {
            parent_slug: doc.slug,
            chunk_ix: chunk.ix,
            start_word: chunk.startWord,
            frontmatter: doc.frontmatter,
            file_path: doc.file_path,
          },
        });
      }

      console.log(`üìù ${doc.slug}: ${chunks.length} chunks`);
    }

    console.log(`\\nüìä Resume ingestion summary:`);
    console.log(`   üìö Documents to process: ${docsToProcess.length}`);
    console.log(`   üß© Total chunks to generate: ${totalChunks}`);
    console.log(`   ‚è≠Ô∏è  Skipped documents: ${existingDocs.size}`);

    // Step 6: Ingest with optimizations
    console.log(`\\nüöÄ Starting optimized parallel ingestion...`);
    await parallelService.ingestWithOptimizations(contentItems, true);

    console.log(`\\n‚úÖ Checkpoint resume completed successfully!`);
    console.log(`üìä Processed ${docsToProcess.length} documents with ${totalChunks} chunks`);

  } catch (error) {
    console.error('‚ùå Checkpoint resume failed:', error);
    process.exit(1);
  }
}

main().catch(error => {
  console.error('üí• Process failed:', error);
  process.exit(1);
});
